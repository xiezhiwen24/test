聚类分析是一种无监督学习，监督学习(supervised learning)：是对具有概念标记（分类）的训练样本进行学习，以尽可能对训练样本集外的数据进行标记（分类）预测。【神经网络和决策树】
                                             无监督学习(unsupervised learning)：是对没有概念标记（分类）的训练样本进行学习，以发现训练样本集中的结构性知识。【聚类】
聚类，就是按照一定的相似性度量方式，把接近的一些个体聚在一起。这里主要是相似性度量，不同的数据类型，我们需要用不同的度量方式。
聚类的思想也很重要，其主要分为五大类，第一大类是基于分割的聚类，比如k-means，以及按照这个思路进行了简单扩展的几个聚类，如k-median等。第二大类是层次聚类，它其实是把个体之间的关系进行了一个层次展示，具体聚为几类，由人为进行设定。第三大类是基于密度的聚类，比如混合模型，就是基于概率分布的聚类，而DBSCAN就是基于密度的聚类，实际上，这里密度是指一指局部密度，而不是概率密度分布。第四大类是基于概率密度分布的聚类，这一类聚类方法主要是假设数据来自某个概率分布，或者是某几个概率分布的组合，进而进行参数估计，确定分布的样子，再反过来看看，样本点属于哪一类。第五大类是矩阵的分解（Nonnegative Matrix Factorizations ），这一大类其实和之前的几类明显不同，比如SVD分解，或者其他的分解其实在文本挖掘或者推荐算法里边都属于聚类。最后一大类就是谱聚类了

K-means算法的思想，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。
1）对于K值，根据先验经验选择一个合适的或者，通过交叉验证选择一个合适的
2）K-means有明显的训练过程，找到K个类别的最佳质心，从而决定样本的簇类别（启发式的迭代法），而KNN是监督学习K-Means的主要优点有：

K-Means的主要优点有：
1）原理比较简单，实现也是很容易，收敛速度快。
2）聚类效果较优。
3）算法的可解释度比较强。
4）主要需要调参的参数仅仅是簇数k。

K-Means的主要缺点有：
1）K值的选取不好把握
2）对于不是凸的数据集比较难收敛
3）如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
4） 采用迭代方法，得到的结果只是局部最优。
5） 对噪音和异常点比较的敏感。

基于R语言的K-means聚类，如果需要对分类等类型的数据进行聚类，则只能自己实现Ｋ-Means算法了，先计算数据距离，然后在编写Ｋ-Means算法进行聚类计算。值得一提的是在R语言中使用edit(kmeans)可以查     看kmeans方法的源代码，可以参照源代码实现定制的Ｋ-Means算法。 余弦相似度与欧式距离。SPSS中，方差分析检验结果越显著的变量，说明对聚类结果越有影响，对于不显著的变量，可以考虑从模型中剔除。
library(factoextra)
# 载入数据
data("USArrests") #美国各州的犯罪率
str(USArrests)  #查看各变量
# 数据进行标准化
df <- scale(USArrests) 
# 查看数据的前五行
head(df, n = 5)
#确定最佳聚类数目,这里函数直观给出最佳分类个数，可以发现聚为四类最合适，当然这个没有绝对的，从指标上看，选择坡度变化不明显的点最为最佳聚类数目。
fviz_nbclust(df, kmeans, method = "wss") + geom_vline(xintercept = 4, linetype = 2)
#设置随机数种子，保证实验的可重复进行
set.seed(123)
#利用k-mean是进行聚类
km_result <- kmeans(df, 4, nstart = 24)  #nstart?
#查看聚类的一些结果
print(km_result)
#提取类标签并且与原始数据进行合并
dd <- cbind(USArrests, cluster = km_result$cluster)
head(dd)
#查看每一类的数目
table(dd$cluster)
#进行可视化展示
fviz_cluster(km_result, data = df,
             palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
             ellipse.type = "euclid",
             star.plot = TRUE, 
             repel = TRUE,
             ggtheme = theme_minimal()
)

基于R语言的层次聚类
系统聚类：也称为层次聚类，分类的单位由高到低呈树形结构，且所处的位置越低，其所包含的对象就越少，但这些对象间的共同特征越多。该聚类方法只适合在小数据量的时候使用，数据量大的时候速度会非常慢。
层次聚类一般有两种划分策略，自底向上的聚合（agglomerative）策略和自顶向下的分拆（divisive）策略。
聚合层次聚类的基本思想：
1）计算数据集的相似矩阵；
2）假设每个样本点为一个簇类；
3）循环：合并相似度最高的两个簇类，然后更新相似矩阵；
4）当簇类个数为1时，循环终止；
簇间相识度的距离计算方法
最小距离、最大距离、平均距离、中心距离、最小方差法
对于文本或非数值型的数据，我们常用汉明距离（Hamming distance）和编辑距离（Levenshtein distance）表示样本间的距离。
数据量较大时，层次聚类算法的空间复杂度和时间复杂度较高，我们可以通过连通性约束（connectivity constraint）降低算法复杂度，甚至提高聚类结果。连通性约束是通过连通性矩阵（connectivity matrix）实现的，连通性矩阵的元素只有1和0两种结果，1表示两个样本是连通的，0表示两个样本不连通，我们只对连通性的样本进行距离计算并融合，这一过程大大降低了计算量，常采用sklearn.neighbors.kneighbors_graph来构建连接矩阵。

层次聚类算法的优缺点
优点：算法简单，易于理解；树状图包含了整个算法过程的信息。缺点：选择合适的距离度量与簇类的链接准则较难；高的时间复杂度和空间复杂度。

#先求样本之间两两相似性
result <- dist(df, method = "euclidean")
#产生层次结构
result_hc <- hclust(d = result, method = "ward.D2")
#进行初步展示
fviz_dend(result_hc, cex = 0.6)
#最终展示
fviz_dend(result_hc, k = 4, 
          cex = 0.5, 
          k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
          color_labels_by_k = TRUE, 
          rect = TRUE          
)












